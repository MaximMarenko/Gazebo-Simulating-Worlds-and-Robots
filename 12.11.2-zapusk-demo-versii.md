# 12.11.2 Запуск демо-версии

_Perception Pipeline_, входящий в состав пакета _ubr1\_grasping_, может быть запущен в качестве автономного ROS-сервера действий для обнаружения объектов обычной формы, таких как коробки и цилиндры, расположенных на плоской поверхности, например, столешнице. Сервер захвата действий восприятия возвращает позы и формы любых обнаруженных объектов, тем самым позволяя использовать результаты по поднятию и размещению с нашим собственным роботом. Разумеется, наш робот должен иметь руку и захват, а также глубинную камеру.

Чтобы увидеть, как это работает, вам понадобится целевой объект, такой как небольшая коробка или цилиндр, расположенный на столешнице в пределах досягаемости руки и захвата вашего робота. Для маленьких захватов хорошо подойдет тюбик с зубной пастой или пузырек для таблеток. Газировку можно использовать, если захват вашего робота открывается достаточно широко.

Сначала прервите любую запущенную симуляцию Gazebo, а также любые специфические узлы UBR-1 или пусковые файлы MoveIt!.

Для начала запустите стартовый файл для вашего робота. В случае с автором была использована слегка модифицированная версия Pi Robot с рукой, опущенной на туловище примерно на 0.3 метра. Это позволяет относительно короткой руке Pi достигать целевых объектов, размещенных на низком столе, расположенном за пределами минимального расстояния 0.5 метра от Kinect. Соответствующий файл запуска для этой модели робота:

```text
$ roslaunch rbx2_bringup grasping_pi_robot_no_base.launch sim:=false
```

Если ваш робот использует сервоприводы Dynamixel и драйвер ArbotiX ROS, такой как Pi Robot, вам, возможно, захочется уберечь сервоприводы от перегрева, запустив узел _Monitor\_dynamixels.py_, который мы создали в главе "Диагностика ROS":

```text
$ roslaunch rbx2_diagnostics monitor_dynamixels.launch
```

Далее откройте узел OpenNI для вашей глубинной камеры:

```text
$ roslaunch rbx2_vision openni_node.launch
```

Напомним, что этот пусковой файл устанавливает разрешение изображения и глубины до 320x240 \(QVGA\), что кажется достаточно хорошим для обнаружения даже относительно небольших целевых объектов, но при этом удерживает нагрузку на процессор на довольно низком уровне. Если вы чувствуете, что вам нужно более высокое разрешение, вы можете использовать _rqt\_reconfigure_ после запуска пускового файла для переключения на VGA.

Запустите узел image\_view, чтобы увидеть то, что видит камера:

```text
$ rosrun image_view image_view image:=/camera/rgb/image_color
```

Убедитесь, что камера находится на высоте пары футов или около того над столом, и наклоните ее вниз так, чтобы она смотрела на целевой объект. В качестве целевого объекта автор использовал небольшую коробку с зубной пастой, а вид через камеру выглядел как изображение ниже вместе с боковым снимком установки:

![](.gitbook/assets/image%20%282%29.png)

![](.gitbook/assets/image%20%283%29.png)

Далее, если вы еще этого не сделали, отключите обработку octomap в MoveIt!, как мы описали в конце раздела 11.27. Генерация и отображение карты заполнения из облака камеры потребляет довольно много энергии процессора, и так как мы не нуждаемся в нем для этого теста, мы получим лучшую производительность, выключив его.

А теперь запустите "MoveIt!" узлы для вашего робота. Для Пи-Робота мы бы ввели:

```text
$ roslaunch grasping_pi_robot_moveit_config move_group.launch
```

Далее запустите _RViz_ с помощью конфигурационного файла _real\_pick\_and\_place.rviz_:

```text
$ rosrun rviz rviz -d `rospack find
rbx2_arm_nav`/config/real_pick_and_place.rviz
```

Для запуска автономного UBR-1 perception pipeline используйте файл _ubr1\_perception.launch_ в пакете _rbx2\_gazebo_:

```text
$ roslaunch rbx2_gazebo ubr1_perception.launch
```

Выходные сообщения должны быть похожи на:

> process\[basic\_grasping\_perception-1\]: started with pid \[440\] \[ INFO\] \[1405723663.137069807\]: basic\_grasping\_perception initialized

Файл _ubr1\_perception.launch_ почти такой же, как и файл grasping.launch в пакете _ubr1\_grasping_, но включает в себя переопределение раздела _/head\_camera_, используемого UBR-1, в раздел _/camera_, используемый файлом _openni\_node.launch_.

UBR-1 perception pipeline создает ROS сервер действий в пространстве имен _basic\_grasping\_perception/find\_objects_, который может быть вызван, чтобы инициировать сегментирование визуальной сцены на опорную поверхность \(или поверхности\) и целевой объект \(или объекты\). В результате получается набор примитивов объектов \(коробок и цилиндров\) и их позы, которые можно вставить в сцену планирования "MoveIt!". Мы можем использовать эти позы, чтобы направить руку робота, чтобы захватить и переместить цель, избегая при этом столкновений со столешницей или другими предметами. Дополнительная информация будет предоставлена после запуска демо-версии.

Теперь мы готовы запустить наш собственный скрипт pick-and-place под названием real\_pick\_and\_place.py, находящийся в каталоге rbx2\_arm\_nav/scripts. Этот скрипт начинается с вызова сервера восприятия UBR-1, чтобы получить формы и позы столешницы и поддерживаемых объектов. Затем он использует эти позы с кодом из нашего предыдущего скрипта moveit\_pick\_and\_place\_demo.py для генерации соответствующих поз для захвата, затем выполняет операции pick\(\) и place\(\). \(Позиция места, используемая для демонстрации, была выбрана таким образом, чтобы робот перемещал объект в место, расположенное слева от туловища и на том же расстоянии перед роботом, что и первоначальная цель\).

Чтобы протестировать perception pipeline, но без попыток переместить руку, запустите сценарий с аргументом --objects.

```text
$ rosrun rbx2_arm_nav real_pick_and_place.py --objects
```

Изначально в окне терминала можно увидеть несколько сообщений, как показано ниже:

> \[INFO\] \[WallTime: 1405913685.062874\] Found 0 objects

Просто отрегулируйте угол наклона камеры либо вверх, либо вниз, пока в идеале не будет обнаружен только один объект:

> \[INFO\] \[WallTime: 1405913640.773310\] Found 1 object

Вернемся в RViz, вид должен выглядеть следующим образом:

![](.gitbook/assets/image%20%284%29.png)

Здесь мы видим, что UBR-1 perception pipeline успешно идентифицировал стол \(показан прозрачным серым цветом\), а также форму и расположение коробки, расположенной сверху \(показана оранжевым цветом\). Обратите внимание, что ящик иногда может выглядеть как цилиндр, а не как прямоугольник \(и наоборот для цилиндрических мишеней\), так как процесс подгонки не является идеальным. По большей части это не повлияет на захват, пока ширина виртуального объекта примерно одинакова в обоих случаях.

Предполагая, что ваш целевой объект может быть обнаружен, Ctrl-C из скрипта real\_pick\_and\_place.py, убедитесь, что ваш робот готов к реальному тестированию, и запустите скрипт снова с аргументом --once. При этом будет сделана попытка выполнить один цикл pick-and-place, чтобы захватить и переместить объект, а затем выйти:

```text
$ rosrun rbx2_arm_nav real_pick_and_place.py --once
```

Если все идет хорошо, робот должен начать с того, что поднимет руку вокруг конца стола, стараясь при этом не касаться стола никакими частями руки. После этого робот протянет руку к целевому объекту и откроет захват, готовясь к захвату. Как только захват переместится в положение вокруг цели, пальцы должны закрыться таким образом, чтобы захватить предмет. На этом операция _pick\(\)_ завершена. Затем скрипт выполняет функцию _place\(\)_, которая пытается найти IK solution, которое переместит захваченный объект в положение, определенное в скрипте. В случае успеха, рука перемещает объект в новое место, опускает на стол и открывает захват, чтобы освободить его. Затем рука оттянется назад и вверх, чтобы отступить от объекта, не перевернув его, прежде чем вернуться в положение покоя, снова избегая любого контакта со столом вдоль ее траектории.

Видео с роботом, проводящим операцию взятия и размещения - [ссылка](https://www.youtube.com/watch?v=Y-jl_m1Q-Bs).

Важно отметить, что после начала работы захвата больше нет визуальной обратной связи, которая направляла бы руку к цели. Другими словами, как только стол и объект находятся у perception pipeline, рука начинает двигаться по заранее запланированной траектории до самого объекта. Это означает, что захват иногда ударяется о цель или вообще не захватывает ее. Аналогично, операция расположения предполагает, что стол находится в том же положении, в котором он был изначально обнаружен. Если бы мы удалили часть стола через движение руки, робот просто уронил бы объект на пол в месте расположения. Наконец, и операции взятия, и операции определения местоположения могут не увенчаться успехом из-за того, что IK solver не смог найти решение. Создание IKFast solver для руки робота обычно повышает надежность поиска решений.





