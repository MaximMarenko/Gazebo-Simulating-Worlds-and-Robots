# 12.11.3 Понимание скрипта real\_pick\_and\_place.py

Как мы уже видели, выполнение pick-and-place с использованием реального робота очень похоже на запуск того же процесса с виртуальными объектами. Основное отличие состоит в том, что теперь нам нужно сегментировать реальную визуальную сцену на опорную поверхность и на объект, который должен взять робот. К счастью, мы смогли использовать UBR-1 perception pipeline для выполнения визуальной обработки для нас. Как только мы получим позы соответствующих объектов на сцене, мы сможем запустить по существу тот же код, который мы использовали в предыдущей главе, где мы подделали pick-and-place с помощью симулятора ArbotiX.

Скрипт real\_pick\_and\_place.py объединяет вывод из UBR-1 perception pipeline с нашим более ранним кодом pick-and-place, найденным в скрипте moveit\_pick\_and\_place\_demo.py. Так как мы уже подробно рассматривали этот скрипт, то остановимся на строках, которые предполагают получение информации об объекте из узла восприятия UBR-1.

```text
import actionlib
from moveit_python import PlanningSceneInterface
from grasping_msgs.msg import *
```

Наверху скрипта _real\_pick\_and\_place.py_ мы импортируем несколько дополнительных библиотек и записей, которые нам понадобятся для работы с узлом UBR-1. Библиотека _actionlib_ необходима для того, чтобы мы могли подключиться к серверу действия UBR-1 _basic\_grasping\_perception_, чтобы получить позы объектов. Из библиотеки _moveit\_python_, созданной Майклом Фергюсоном, нам нужен интерфейс _PlanningSceneInterface_, включающий обёртки Python для ряда утилит MoveIt! для добавления и удаления объектов сцены. Пакет _grasping\_msgs_ используется с сервером действий восприятия для запуска визуальной сегментации и получения результатов, как мы увидим далее.

```text
find_objects =
actionlib.SimpleActionClient("basic_grasping_perception/find_objects",
FindGraspableObjectsAction)
```

Здесь мы присваиваем _SimpleActionClient_ переменной _find\_objects_, которая подключается к серверу действий UBR-1 _basic\_grasping\_perception/find\_objects_ и использует тип сообщения _FindGraspableObjectsAction_, найденный в пакете _grasping\_msgs_. Мы используем этот клиент действия для инициирования сегментации визуального изображения, которое вернёт позиции стола и целевого объекта.

```text
goal = FindGraspableObjectsGoal()

# We don't use the UBR-1 grasp planner as it does not work with our gripper
goal.plan_grasps = False

# Send the goal request to the find_objects action server which will trigger
# the perception pipeline
find_objects.send_goal(goal)

# Wait for a result
find_objects.wait_for_result(rospy.Duration(5.0))

# The result will contain support surface(s) and objects(s) if any are detected
find_result = find_objects.get_result()
```

Чтобы вызвать perception pipeline, мы сначала создаем пустой _goal_ объект, используя тип сообщения _FindGraspableObjectsGoal_. Затем мы отключаем планировщик захвата UBR-1, который специфичен для параллельного захвата UBR-1 \(Мы будем использовать наш собственный генератор захвата, как мы делали это ранее с фальшивым захватом\). Затем мы посылаем пустую цель на сервер действий, используя наш клиент действия _find\_objects_, который запускает процесс визуальной сегментации на сервере действий. Затем мы ждем результаты до 5 секунд и сохраняем все, что возвращается, в переменной _find\_results_. Эти результаты будут представлены в виде сообщения _FindGraspableObjectsResult_, содержащего массу информации о форме и положении любых обнаруженных опорных поверхностей и объектов. Полное определение сообщения можно увидеть, выполнив команду:

```text
$ rosmsg show grasping_msgs/FindGraspableObjectsResult
```

Теперь, когда у нас есть результаты распознавания, мы можем получить позиции опорной поверхности и целевого объекта.

```text
 for obj in find_result.objects:
count += 1
         scene.addSolidPrimitive("object%d"%count,
obj.object.primitives[0],
obj.object.primitive_poses[0],
                             =wait = False)
```

Этот цикл проходит через все обнаруженные объекты в результатах и добавляет их формы и позиции в сцену планирования MoveIt!.

```text
    # Choose the object nearest to the robot
dx = obj.object.primitive_poses[0].position.x - args.x
dy =
obj.object.primitive_poses[0].position.y d =
math.sqrt((dx * dx) + (dy * dy)) if d <
the_object_dist: the_object_dist = d
the_object = count

    # Get the size of the target
target_size = obj.object.primitives[0].dimensions

    # Set the target pose target_pose.pose =
obj.object.primitive_poses[0]
```

Проходя через объекты, мы следим за тем, что находится ближе всего к роботу. Это станет нашей целью. Размер мишени мы получаем из размеров примитива объекта \(например, коробки, цилиндра и т.д.\) и позиции из массива primitive\_poses объекта.

```text
    target_pose.pose.orientation.x = 0.0
target_pose.pose.orientation.y = 0.0
target_pose.pose.orientation.z = 0.0
target_pose.pose.orientation.w = 1.0
```

Так как мы обычно хотим, чтобы рука приближалась к мишени с горизонтально ориентированным захватом, мы устанавливаем положение мишени соответствующим образом. Обратите внимание, что вы также можете попробовать использовать позу объекта непосредственно для позы цели.

Аналогичный цикл в скрипте \(который мы здесь повторять не будем\) циклически проходит через любые обнаруженные поверхности опоры и вставляет их в сцену планирования, как мы это делали для объектов. В нашей простой установке есть только одна поверхность опоры, так что мы можем предположить, что это столешница находится перед роботом.

```text
place_pose = PoseStamped()
place_pose.header.frame_id = REFERENCE_FRAME place_pose.pose.position.x =
target_pose.pose.position.x
place_pose.pose.position.y = 0.03
place_pose.pose.position.z = table_size[2] + target_size[2] / 2.0 + 0.015
place_pose.pose.orientation.w = 1.0
```

Здесь мы определяем _place\_pose_, которое является позицией и ориентацией, в которую мы хотим переместить объект после его захвата. В этом случае мы хотим, чтобы объект оказался на таком же расстоянии перед роботом \(_position.x_\), затем 3 см слева от туловища \(_position.y_\) и на поверхности стола \(_position.z_\), которую мы вычисляем из высоты стола, половины высоты цели и коэффициента калибровки 0,015 метра вверх просто для того, чтобы быть уверенными, что мы не попытаемся вставить объект внутрь стола.

```text
    # Initialize the grasp pose to the target pose
grasp_pose = target_pose

    # Shift the grasp pose half the size of the target to center it in the
gripper try:     grasp_pose.pose.position.x += target_size[0] / 2.0
grasp_pose.pose.position.y -= 0.01
    grasp_pose.pose.position.z += target_size[2] / 2.0
except:
    rospy.loginfo("Invalid object size so
skipping")     continue

# Generate a list of grasps grasps =
self.make_grasps(grasp_pose, [target_id])
```

Последний шаг перед началом работы с взятием заключается в том, чтобы установить исходное положение захвата и сгенерировать набор альтернативных захватов, которые можно попробовать с помощью проектировщика взятия. Здесь мы инициализируем позу захвата до целевой позиции. Затем мы перемещаем позицию на половину целевого размера в размерах x и z, чтобы лучше отцентрировать захват на объекте. Боковое смещение в размере y -1,0 см было определено эмпирическим путем, чтобы отцентрировать объект между пальцами захвата.

Остальная часть сценария по сути такая же, как и скрипт moveit\_pick\_and\_place\_demo.py, который мы подробно обсудили в разделе 11.26.

